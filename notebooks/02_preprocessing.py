# -*- coding: utf-8 -*-
"""02_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MarkLewkowskii/Customer_Outflow/blob/main/notebooks/02_preprocessing.ipynb
"""
import joblib
# Завнтажуємо дані
import pandas as pd

#Завантаження даних
file_path = 'data/raw/internet_service_churn.csv'
data = pd.read_csv(file_path)

#Первинний огляд даних
print(data.head(5))
print(data.shape)

"""***Обробка пропусків***"""

#Аналіз відсутніх значень у наборі даних
missing_data = data.isnull().sum()

#Обчислення відсотка відсутніх значень для кожного стовпчика
missing_percentage = (missing_data / len(data)) * 100

#Відображення стовпців з відсутніми значеннями та їх відсотки
missing_info = pd.DataFrame({"Missing Values": missing_data, "Percentage": missing_percentage})
missing_info[missing_info["Missing Values"] > 0]

"""**Було обрано заповнення медіанами, тому що цей підхід дозволяє зберегти дані без значного внеску в статистику. Якщо б пропущені значення видалили, це б вплинуло на логіку аналізу, оскільки дані могли бути критичними для розуміння клієнтів.**"""

# Список колонок для заповнення пропусків медіанами
columns_to_fill = ['download_avg', 'upload_avg']

# Заповнюємо пропуски медіанами
for column in columns_to_fill:
    data[column] = data[column].fillna(data[column].median())

"""**Виходячи з аналізу даних, для користувачів з відтоком (churn = 1) значення remaining_contract мають схильність до низьких значень, тоді як для користувачів без відтоку (churn = 0) значення більше розподілені й ближчі до середніх значень, тому ми будемо заповнювати їх за допомогою групового розподілу, використовуючи окремо медіани для churn = 1 та для churn = 0**"""

# Окрема обробка для 'remaining_contract', із-за великої кількості пропущених даних і виявлення впливу на цільову змінну

target = 'remaining_contract'  # Змінна з пропусками
grouping_var = 'churn'  # Змінна для групування

# Функція для заповнення пропусків медіаною у кожній групі
def fill_missing_by_group(data, target, grouping_var):
    for group in data[grouping_var].unique():
        group_median = data.loc[data[grouping_var] == group, target].median()
        data.loc[(data[grouping_var] == group) & (data[target].isnull()), target] = group_median
    return data

# Виклик функції для заповнення пропусків
data = fill_missing_by_group(data, target, grouping_var)

print("Пропущені значення заповнені медіаною окремо для кожної групи.")

# Перевірка, що відсутніх значень більше немає
missing_values_count = data.isnull().sum()
print(missing_values_count)

"""***Закодувати категоріальні змінні***

**У датасеті всі потенційно категоріальні змінні is_tv_subscriber, is_movie_package_subscriber, download_over_limit, churn вже представлені як числові бінарні змінні 0 або 1.
Це означає, що вони не потребують додаткового кодування.**

***Провести нормалізацію/стандартизацію числових ознак***

**У нашому випадку стандартизація буде більш доречною для забезпечення збалансованості вкладу різних ознак.**
"""

from sklearn.preprocessing import StandardScaler

#Визначення числових змінних
numerical_columns = ['subscription_age', 'bill_avg', 'remaining_contract', 'download_avg', 'upload_avg']

#Ініціалізація StandardScaler
scaler = StandardScaler()

#Стандартизація
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

#Вивід результату
print(data.head())

#Визначення числових змінних
numerical_columns = ['subscription_age', 'bill_avg', 'remaining_contract', 'download_avg', 'upload_avg']
# Перевірка правильності
print(data[numerical_columns].mean()) #Середнє значення
print(data[numerical_columns].std()) #Стандартне відхилення

# Збереження параметрів StandardScaler для подальшого використання
scaler_path = 'data/processed/scaler.pkl'
joblib.dump(scaler, scaler_path)
print(f"Параметри StandardScaler збережено у файл: {scaler_path}")

#Збереження підготовлених данних
data.to_csv('data/processed/processed_data.csv', index=False)